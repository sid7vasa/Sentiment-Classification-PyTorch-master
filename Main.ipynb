{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_letters = string.ascii_letters + \" .,;'-\"\n",
    "df = pd.read_csv(\"./data/IMDB Dataset.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    text = re.sub(cleanr, ' ', text)\n",
    "    text = text.lower()\n",
    "    text = regex.sub(' ', text)\n",
    "    text = ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "    return text\n",
    "def word_process(text, stopwords = False):\n",
    "    text = text.split(\" \")\n",
    "    if stopwords : text = [word for word in text if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "def encoding(text):\n",
    "    text = text.split(\" \")\n",
    "    text = [word2index[word] for word in text]\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production    the filming t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production    the filming t...  positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df.copy()\n",
    "text['review'] = df['review'].apply(preprocess)\n",
    "text.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100030\n"
     ]
    }
   ],
   "source": [
    "all_words = word_process(\" \".join(list(text['review'])))\n",
    "words = list(set(all_words))\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words = Counter(all_words)\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {w:i for i, (w,c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = list(text['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28, 4, 1, 77, 2038, 46, 1050, 11, 100, 149, 41, 3061, 394, 20, 230, 29, 3174, 32, 25, 203, 14, 10, 6, 613, 47, 592, 17, 68, 1, 87, 148, 11, 3217, 68, 44, 3061, 13, 90, 5322, 2, 14820, 135, 4, 559, 61, 265, 8, 203, 37, 1, 647, 141, 1722, 68, 10, 6, 23, 3, 116, 16, 1, 7810, 2312, 40, 11302, 10, 116, 2571, 56, 5848, 17, 5442, 5, 1452, 371, 40, 559, 90, 6, 3784, 8, 1, 355, 356, 4, 1, 647, 7, 6, 433, 3061, 14, 11, 6, 1, 11473, 357, 5, 1, 14535, 6752, 2517, 1031, 50211, 7, 2684, 1399, 22, 22659, 518, 34, 4620, 2439, 4, 1, 1180, 115, 30, 1, 6931, 27, 2881, 11786, 2, 385, 50212, 36, 16327, 6, 23, 297, 22, 1, 4836, 2907, 518, 6, 340, 5, 107, 24380, 8063, 39249, 14536, 4993, 7691, 2426, 2, 52, 36, 43676, 324, 8981, 7236, 12281, 2, 8579, 31241, 25, 112, 223, 240, 9, 60, 132, 1, 280, 1315, 4, 1, 116, 6, 680, 5, 1, 192, 11, 7, 266, 115, 77, 274, 572, 21, 2982, 816, 182, 1287, 4124, 16, 2475, 1213, 816, 1418, 816, 863, 3061, 152, 21, 938, 184, 1, 87, 394, 9, 123, 209, 3217, 68, 14, 36, 1603, 7, 13, 2216, 9, 411, 21, 132, 9, 13, 1569, 16, 7, 18, 14, 9, 290, 52, 9, 1402, 3, 1253, 16, 3061, 2, 190, 10035, 5, 1, 297, 2020, 4, 2120, 559, 23, 41, 559, 18, 7574, 7080, 4961, 35, 230, 29, 2957, 43, 16, 3, 22660, 6796, 35, 230, 494, 22, 627, 2, 75, 240, 17, 7, 70, 7516, 639, 694, 6796, 109, 648, 83, 1180, 18826, 680, 5, 66, 564, 4, 890, 2000, 40, 1180, 549, 149, 3061, 20, 197, 426, 3801, 17, 47, 6, 3285, 795, 1516, 45, 20, 50, 75, 8, 1197, 17, 126, 4058, 479], [3, 389, 120, 350, 1, 1364, 2950, 6, 53, 17438, 53, 158, 57, 2144, 1566, 2, 409, 3, 13069, 2, 525, 25399, 281, 4, 1816, 5, 1, 439, 410, 1, 150, 25, 557, 70, 2251, 481, 4125, 23, 62, 46, 190, 30, 1, 62088, 18, 24, 46, 30, 1, 2267, 176, 3249, 97, 20, 50, 367, 65, 1, 13070, 777, 9581, 33, 1, 1814, 5, 1713, 7237, 6514, 23, 62, 6, 7, 70, 276, 1, 149, 18, 7, 6, 3, 35993, 404, 2, 2377, 410, 3, 4291, 350, 44, 28, 4, 1, 80, 1127, 12, 4, 200, 2, 26, 113, 1, 1816, 64, 270, 340, 17, 1, 120, 179, 1, 1005, 4, 1, 2862, 61, 248, 72, 356, 1, 2179, 952, 3100, 1268, 1173, 93, 4876, 7, 298, 22, 256, 1813, 2, 256, 4513, 574, 17, 1, 135, 3666, 18330, 2, 24381, 2, 1, 717, 574, 4, 66, 1034, 17, 24381, 12, 43677, 26584, 171, 2271, 25, 1951, 70, 219], [9, 191, 10, 13, 3, 389, 95, 5, 1138, 57, 22, 3, 97, 854, 1450, 2572, 1222, 8, 1, 883, 14537, 756, 2, 149, 3, 633, 2312, 200, 1, 111, 6, 4059, 18, 1, 408, 6, 1890, 2, 1, 101, 25, 1465, 59, 1, 70, 6010, 6604, 1540, 470, 136, 48, 197, 29, 667, 51, 32, 945, 10, 6, 23, 1006, 217, 2893, 5102, 9, 191, 7, 13, 3018, 11, 2812, 1767, 6, 131, 1385, 8, 1115, 4, 1, 393, 107, 4, 178, 27, 2039, 5, 110, 10, 13, 1, 89, 9, 222, 1424, 31, 28, 4, 2812, 12, 1291, 8, 154, 2982, 9, 132, 3, 2040, 136, 9, 137, 112, 76, 1489, 17, 8126, 29444, 8, 10, 54, 1312, 5, 1238, 176, 42, 1230, 1404, 2, 5035, 203, 83, 3, 830, 18, 3514, 186, 245, 10, 197, 23, 29, 1, 7131, 5056, 4, 26, 614, 18, 7, 13, 29445, 72, 1799, 2908, 23464, 2, 52, 218, 72, 2490, 3, 80, 200, 5, 141, 65, 17, 348]]\n"
     ]
    }
   ],
   "source": [
    "reviews_encoded = []\n",
    "for review in reviews:\n",
    "    r = [vocab_to_int[w] for w in review.split()]\n",
    "    reviews_encoded.append(r)\n",
    "print (reviews_encoded[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_lens = [len(enc) for enc in reviews_encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2492"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(review_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234.08298"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(review_lens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(text['sentiment'])\n",
    "encoded_labels = [1 if label =='positive' else 0 for label in labels]\n",
    "encoded_labels = np.array(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_encoded = [ reviews_encoded[i] for i, l in enumerate(review_lens) if l<500 ]\n",
    "encoded_labels = np.array([ encoded_labels[i] for i, l in enumerate(review_lens) if l< 500 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45957, 45957)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_encoded), len(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_int, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
    "    '''\n",
    "    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n",
    "    \n",
    "    for i, review in enumerate(reviews_int):\n",
    "        review_len = len(review)\n",
    "        \n",
    "        if review_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-review_len))\n",
    "            new = zeroes+review\n",
    "        elif review_len > seq_length:\n",
    "            new = review[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pad_features(reviews_encoded, 500)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.8\n",
    "len_feat = features.shape[0]\n",
    "train_x = features[0:int(split_frac*len_feat)]\n",
    "train_y = encoded_labels[0:int(split_frac*len_feat)]\n",
    "remaining_x = features[int(split_frac*len_feat):]\n",
    "remaining_y = encoded_labels[int(split_frac*len_feat):]\n",
    "valid_x = remaining_x[0:int(len(remaining_x)*0.5)]\n",
    "valid_y = remaining_y[0:int(len(remaining_y)*0.5)]\n",
    "test_x = remaining_x[int(len(remaining_x)*0.5):]\n",
    "test_y = remaining_y[int(len(remaining_y)*0.5):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch Datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "# make sure to SHUFFLE your data\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 500])\n",
      "Sample input: \n",
      " tensor([[   0,    0,    0,  ..., 1322,    1, 2164],\n",
      "        [   0,    0,    0,  ...,  526,   44,    7],\n",
      "        [   0,    0,    0,  ...,  726,    4, 4649],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 5531,    5,  845],\n",
      "        [   0,    0,    0,  ..., 1306,    8, 3845],\n",
      "        [   0,    0,    0,  ...,    3,   49,   28]], dtype=torch.int32)\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length # batch_size sentences at once\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size # batch_size labels at once\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentLSTM(\n",
      "  (embedding): Embedding(100031, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 100... Loss: 0.139125... Val Loss: 0.318243\n",
      "Epoch: 1/4... Step: 200... Loss: 0.282121... Val Loss: 0.282419\n",
      "Epoch: 1/4... Step: 300... Loss: 0.440695... Val Loss: 0.319842\n",
      "Epoch: 1/4... Step: 400... Loss: 0.413789... Val Loss: 0.365040\n",
      "Epoch: 1/4... Step: 500... Loss: 0.322951... Val Loss: 0.362384\n",
      "Epoch: 1/4... Step: 600... Loss: 0.276541... Val Loss: 0.307061\n",
      "Epoch: 1/4... Step: 700... Loss: 0.150527... Val Loss: 0.281822\n",
      "Epoch: 2/4... Step: 800... Loss: 0.185218... Val Loss: 0.279230\n",
      "Epoch: 2/4... Step: 900... Loss: 0.036196... Val Loss: 0.270710\n",
      "Epoch: 2/4... Step: 1000... Loss: 0.117091... Val Loss: 0.272639\n",
      "Epoch: 2/4... Step: 1100... Loss: 0.227487... Val Loss: 0.304202\n",
      "Epoch: 2/4... Step: 1200... Loss: 0.155751... Val Loss: 0.268672\n",
      "Epoch: 2/4... Step: 1300... Loss: 0.156565... Val Loss: 0.263972\n",
      "Epoch: 2/4... Step: 1400... Loss: 0.098875... Val Loss: 0.268842\n",
      "Epoch: 3/4... Step: 1500... Loss: 0.096785... Val Loss: 0.284112\n",
      "Epoch: 3/4... Step: 1600... Loss: 0.155341... Val Loss: 0.308042\n",
      "Epoch: 3/4... Step: 1700... Loss: 0.090226... Val Loss: 0.296792\n",
      "Epoch: 3/4... Step: 1800... Loss: 0.096162... Val Loss: 0.303571\n",
      "Epoch: 3/4... Step: 1900... Loss: 0.248851... Val Loss: 0.305708\n",
      "Epoch: 3/4... Step: 2000... Loss: 0.076228... Val Loss: 0.299751\n",
      "Epoch: 3/4... Step: 2100... Loss: 0.058314... Val Loss: 0.280393\n",
      "Epoch: 3/4... Step: 2200... Loss: 0.211039... Val Loss: 0.277467\n",
      "Epoch: 4/4... Step: 2300... Loss: 0.166594... Val Loss: 0.328002\n",
      "Epoch: 4/4... Step: 2400... Loss: 0.064674... Val Loss: 0.363764\n",
      "Epoch: 4/4... Step: 2500... Loss: 0.014868... Val Loss: 0.324054\n",
      "Epoch: 4/4... Step: 2600... Loss: 0.048148... Val Loss: 0.363176\n",
      "Epoch: 4/4... Step: 2700... Loss: 0.052066... Val Loss: 0.322874\n",
      "Epoch: 4/4... Step: 2800... Loss: 0.082443... Val Loss: 0.334884\n",
      "Epoch: 4/4... Step: 2900... Loss: 0.069704... Val Loss: 0.333854\n"
     ]
    }
   ],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "train_on_gpu = True\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "count = 0 \n",
    "v_count = 0\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        inputs = inputs.type(torch.LongTensor)\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        count = count + 1\n",
    "        if inputs.shape[0] != batch_size : continue\n",
    "\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs = inputs.type(torch.LongTensor)\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                if inputs.shape[0] != batch_size : continue\n",
    "                v_count = v_count + 1\n",
    "                \n",
    "                \n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.440\n",
      "Test accuracy: 0.865\n"
     ]
    }
   ],
   "source": [
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs = inputs.type(torch.LongTensor)\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    if inputs.shape[0] != batch_size : continue\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[90, 3, 74, 15]]\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0 90  3 74 15]]\n",
      "torch.Size([1, 200])\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def tokenize_review(test_review):\n",
    "    test_review = test_review.lower() # lowercase\n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
    "\n",
    "    return test_ints\n",
    "\n",
    "# test code and generate tokenized review\n",
    "test_ints = tokenize_review(\"It's a bad movie\")\n",
    "print(test_ints)\n",
    "\n",
    "\n",
    "# test sequence padding\n",
    "seq_length=200\n",
    "features = pad_features(test_ints, seq_length)\n",
    "\n",
    "print(features)\n",
    "\n",
    "\n",
    "# test conversion to tensor and pass into your model\n",
    "feature_tensor = torch.from_numpy(features)\n",
    "print(feature_tensor.size())\n",
    "\n",
    "\n",
    "def predict(net, test_review, sequence_length=200):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # tokenize review\n",
    "    test_ints = tokenize_review(test_review)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    feature_tensor = feature_tensor.type(torch.LongTensor)\n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    # printing output value, before rounding\n",
    "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "    \n",
    "    # print custom response\n",
    "    if(pred.item()==1):\n",
    "        print(\"Positive review detected!\")\n",
    "    else:\n",
    "        print(\"Negative review detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.066403\n",
      "Negative review detected.\n"
     ]
    }
   ],
   "source": [
    "review_input = \"There are a tons of characters, too many to follow. I keep forgetting who is who and is related to who. And then the multiple timelines make everything even more confusing. Not that it matters because there are basically no interesting storylines to follow.\"\n",
    "predict(net, review_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
